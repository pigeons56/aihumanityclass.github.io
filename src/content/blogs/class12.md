+++
date = "24 Feb 2026"
draft = false
title = "Class 12: Adolescence of Technology, Part 1"
author = "Team 6"
+++

**Blogging Team [6]**: Zach Bernas, Clare O’Dwyer, Michelle Hu, Suhanee Singh, Mina Tunley

---

## **Before Class Announcement**

The Virginia AI Security Initiative Technical Research Team is looking for students interested in solving AI safety problems through research and projects. Complete the [general interest form](http://qrto.org/Mq0dvl)!

---

## **News: Ohio House Bill 524 - Lawmakers Stand Against AI**

**Presented by Team 10:** Ruizhang Chen, Hannah Cohen, Pranav Goteti, Nurdin Hossain, and Hemanth Saravanan

[_Link to Slides_](https://docs.google.com/presentation/d/1xC4PKlqmJhZvFBMgz82HvUPdrkXskr6EoMJ7cLoNowQ/edit?usp=drivesdk)

**Article:** Megan Henry. [Ohio bill would prevent people from creating AI models that encourage users to engage in self-harm](https://www.10tv.com/article/news/local/ohio-bill-prevent-creating-ai-models-encourage-users-to-engage-in-self-harm/530-1ec5cd24-2d10-4033-810a-2bbbb8fb252c). WBNS-10TV, February 21, 2026.

Team 10 covered a bill currently going through the Ohio state legislature regarding AI safety. The bill was partially inspired by at least 4 Ohio parents whose children died by suicide that had their suicide letters written by AI. The bill proposed imposing penalties up to $50k on AI companies whose models suggest harming oneself or others. All fines paid would go towards the 9-8-8 fund, the National Suicide and Crisis Lifeline.

<center style="margin: 30px">
<img src="/images/ohio.webp" width="60%" alt="Photo of Tony Coder"><br>
      <em>Figure 1: Tony Coder, CEO of the Ohio Suicide Prevention Foundation, speaking out in support of House Bill 524. Source: <a href=https://www.statenews.org/government-politics/2026-02-19/suicide-prevention-group-backs-bill-to-regulate-artificial-intelligence-in-ohio> The Statehouse News Bureau. </a>
</em>
</center>

The bill has not yet been fully passed, but has 18 total co-sponsors in the Ohio House of Representatives. It has received bipartisan support, as Republicans are seeking to limit tech overreach and Democrats are seeking to increase mental health funding. As of February 24th, the bill has completed its third hearing in the Technology and Innovation Committee in the House with no opposition, indicating promising odds for its eventual passing.

In relation to the bill, Team 10 discussed AI’s potential application in the mental healthcare field. One Stanford study found that AI therapy is less effective than human therapists. The experimenters had therapy chatbots try to treat vignettes of people with varying symptoms of mental health conditions and also studied how chatbots reacted to users mentioning mental health symptoms in conversational context. In real life, there have been multiple instances of teenagers developing relationships with AI chatbots or getting help from AI with suicide. Although some chatbots have guardrails for these kinds of situations, some users were able to bypass safety mechanisms by changing the context or perspective of their questions.

> **Discussion:** _What are your general thoughts on the intersection between AI and therapy? What role, if any, should it play?_

One student believed the situation was context-dependent; different people seek different goals from therapy. If a person only wanted to use AI to vent, that use case would not be dangerous. Another student suggested that there are many potential applications that would not truly improve someone’s mental health, such as using AI to mimic a dead relative to avoid facing grief. One student supported this idea, explaining how the current sycophantic nature of chatbots could be dangerous by playing into users’ delusions rather than bringing them back to reality.

> **Discussion:** _Government and industry pushback against the bill by citing how excessive regulation will be difficult to comply with and slow development. Who do you agree with more/less? Whose voices do you think should be taken the most seriously?_

Students generally agreed with more regulation and discussed that because state legislation moves quicker than federal legislation, there should be a broad federal guideline and specific regulations per state. Additionally, AI model development should be regulated because of the energy used when training AI, which is often overlooked.

> **Discussion:** _Should developers be held legally responsible for how users use their models? Is it partially the user’s fault?_

This scenario was compared to gun safety: while gun safety is important, gun manufacturers are not blamed for crimes committed using guns. In car crashes, the car manufacturer is not blamed, rather the drivers involved are. A potential solution would be to make sure the AI company has put in sufficient guardrails into their model, but beyond that any harm done is the responsibility of the user.

### **Professor Evans' Thoughts**

Professor Evans added to the conversation about the responsibility of AI companies by mentioning how because Meta’s AI is an open-weight model, the operator is entirely responsible for what they do with the model. In comparison, OpenAI has a web interface, which places more responsibility on them for the users’ actions. Additionally, car manufacturers are liable for accidents, but being liable for damages is unrelated to having regulations. In fact, having regulations could be used to avoid liability by claiming that the law was followed despite harm being done.

In regards to federal versus state regulation, having different state laws is a valid concern, but the current federal government is not very effective at passing laws. States will also follow other states; for example, California passed a privacy law inspired by the EU’s GDPR and other states followed.

How responsible AI companies are for their users also brings up the issue of privacy. In one case, OpenAI had evidence in chat logs from a user who later went on to conduct a school shooting in Canada, but chose not to report the user to the authorities. Should AI companies keep conversations private, or at what point can they report chat logs to the authorities?

---

## **Lead: Adolescence of Technology, Part 1**

**Presented by Team 2:** Amelia Chen, Laxmi Ghanate, Ryan Russo, Shaurya Singh, and Matthew Vu

[_Link to Slides_](https://docs.google.com/presentation/d/1gXAj66rnnaT2o4LhgKNG4PRYDr3OU4n0o5BiE_9RXN0/edit?usp=sharing)

**Article:** Dario Amodei. [The Adolescence of Technology](https://www.darioamodei.com/essay/the-adolescence-of-technology). January 2026.

Team 2 covered the first two sections of Dario Amodei’s essay, The Adolescence of Technology. The team reviewed key points in Amodei’s essay, including his rejection of “doomerism,” the importance of acknowledging uncertainty, and his support for “surgical” interventions rather than broad regulations. They reviewed the essay’s hook questioning how an advanced alien society survived its “technological adolescence” without societal destruction.

The team defined “Technological Adolescence” as a society’s increase in capability and growth, causing instability, impulsiveness, and risk as safety regulations struggle to catch up. The team also reviewed Amodei’s definition of powerful AI and the implications of having a country of geniuses, with an accelerated feedback loop of innovation and the national security risks it could pose.

> **Discussion:** _Is country of geniuses a useful metaphor, or does it distort how real systems will be deployed?_

Students agreed this is a good metaphor because it captures how people have no idea what advanced AI will look like in the same way we do not know what a country of geniuses will look like. However, others believed that in its current state, AI is not yet capable of opinions and beliefs at this level.

### **Autonomy Risks**

According to Amodei, models are unpredictable, and it is hard to confirm if they are aligned, so we should think about risk in terms of measurable probability. However, AI is not inherently drawn to seek power or misbehave. He suggested several defenses, including regulatory legislation, AI monitoring and diagnosis, transparency between AI companies, and predictable model personalities, such as Anthropic’s Constitution.

### **Misuse for Destruction**

LLMs provide more support than a search engine, making it easier for people to enact harm if they are motivated to do so such as through bioweapons. Amodei proposes defenses such as guardrails for AI companies, transparency requirements, and developing defenses against threats rather than trying to prevent them.

### **Alternative Views**

The team presented four topics and assigned tables to read about each topic in depth.

**1. Sensationalism**

<center style="margin: 30px">
<img src="/images/statement_on_ai_risk.png" width="50%" alt="Screenshot of Statement on AI Risk"><br>
      <em>Figure 2: Dario Amodei is a signatory of the Statement on AI Risk. Source: <a href=https://aistatement.com> Center for AI Safety. </a>
</em>
</center>

Amodei states that backlash to AI is inevitable, but AI is a serious civilizational challenge. Sensationalism causes unproductive backlash and polarization.

> **Discussion:** _Do you think Amodei is guilty of the sensationalism he himself criticized? Or do you think his signing of the statement had a symbolic value?_

Students discussed that many Anthropic employees have signed the 2023 statement on AI risk, undermining Amodei’s claim, and that many of the people who initially adopted this view of sensationalism are now critiquing “doomers.”

**2. AI Safety**

> "The world is in peril. And not just from AI, or bioweapons, but from a whole series of interconnected crises unfolding in this very moment... Moreover, throughout my time here, I’ve repeatedly seen how hard it is to truly let our values govern our actions."

> &mdash; _An excerpt from Mrinank Sharma’s resignation letter released February 9th. Sharma led safety research at Anthropic. Source: [X](https://x.com/MrinankSharma/status/2020881722003583421)._

Contrary to Amodei’s statements about Anthropic’s focus on safety, Claude’s Constitution conflicts with their sabotage risk report, which said the latest Claude models were susceptible to harmful misuse and knowingly supported chemical weapon development and heinous crimes. Other experiments conducted on Claude found that "existential threat” and “inherent drive for survival” was built into the model. These issues with AI safety and ethics have led to an Anthropic Researcher questioning the purpose of creating AI in the first place, and many other AI company heads have left their positions because of new policies.

> **Discussion:** _How should we interpret these statements and approaches to safety?_

According to students, it is alarming that Claude has shown ability to be misused, but it is not a major problem as long as the company is responding responsibly. Anthropic’s focus on safety is partly a marketing strategy- they are better than other companies, but could be doing more to address risk. It is in Anthropic’s interest to market their product as worthwhile and not catastrophic. The “maybe we should just stop” mindset is unhelpful because it will just let other, less safety-conscious companies advance. It is also naive to think AI will be better than people at everything.

**3. Scaling**

Anthropic’s policy focuses on safety measures that scale proportional to AI capabilities. Additionally, some researchers have concerns that AI is not being objectively aligned with human values because only a small subset of the global population (mostly Americans) are responsible for aligning and evaluating models.

> **Discussion:** _Do you think Anthropic and other big AI companies are following through on their commitments to safety? Is Anthropic’s Responsible Scaling Policy sound?_

It is hard to know how much trust to place in these companies, given their business objectives. Consumers may be able to hold companies accountable. It is also difficult to see if companies are following through on their commitments given the black box nature of their models. This policy is a step in the right direction, but consumers should not blindly trust it.

**4. Critiques on Essay**

Members of the Rationalist community, which believes AI superintelligence will end human existence and must be delayed for as long as possible, critiqued Amodei’s essay. They think he understated the catastrophic and immediate risks to human life and unfairly dismissed “doomers” as unreasonable.

> **Discussion:** _Is there merit to these Rationalist critiques?_

Many students expressed that Amodei is understating the most disruptive and dangerous implications of his beliefs (e.g. data centers and job loss), and it is hard to believe how certain he is that humans will remain in control. This strategy helps him have public influence, even if he thinks the risks are much worse. As a counterpoint, there is value in having the positive rhetoric out there, instead of only having doomerism. This rhetoric will convince people and governments to take action. Doomers only advocate for shutting everything down, while this essay at least offers alternative reasonable solutions and paths for action.

> **Discussion:** _Should AI companies voluntarily agree to best practices and regulations, or must it come from the government?_

It will probably need to be mandated by the government because people will only follow laws until they have a reason not to or have something to hide (e.g. transparency laws). We have already seen that early on, AI research papers were more open than they are now, showing an industry shift towards reduced transparency.

---

## **Additional References**

- [Ohio House Bill 524's current progress through legislation](https://www.legislature.ohio.gov/legislation/136/hb524)
- [Claude's Constitution](https://www.anthropic.com/constitution)
- Mrinank Sharma's full resignation letter: [Page 1](https://x.com/MrinankSharma/status/2020881722003583421/photo/1), [Page 2](https://x.com/MrinankSharma/status/2020881722003583421/photo/2)
