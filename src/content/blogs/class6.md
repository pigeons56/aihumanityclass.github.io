+++
date = "03 Feb 2026"
draft = true
title = "Class 6: Clinical Use of AI in Medicine"
author = "Team 12"
+++

**Blogging Team 12**: Spencer Cook, Qiaojing Huang, Sonika Modur, Witt Smith, Kayla Sprincis

**Lead Topic: Clinical Use of AI in Medicine**

Reading: _GPT-4 assistance for improvement of physician performance on patient care task: a randomized control trial_**. Source: [Nature Medicine](https://www.nature.com/articles/s41591-024-03456-y)


# News: OpenClaw

**Presented by Team 4: [Slides](/docs/class6_news_.pdf)**

## Articles

- _A Social Network for A.I. Bots Only. No Humans Allowed_, Link: [NYTimes](https://www.nature.com/articles/s41591-024-03456-y)

- _Inside Moltbook: The Social Network Where AI Agents Talk And Humans Just Watch_, Link: [Forbes](https://www.forbes.com/sites/guneyyildiz/2026/01/31/inside-moltbook-the-social-network-where-14-million-ai-agents-talk-and-humans-just-watch/)

- _The risky AI assistant clawing its way to viral fame_, Link: [Morning Brew](https://www.morningbrew.com/stories/2026/01/29/the-risky-ai-assistant-clawing-its-way-to-viral-fame)

- _OpenClaw (a.k.a. Moltbot) is everywhere all at once, and a disaster waiting to happen_, Link: [Marcus on AI](https://garymarcus.substack.com/p/openclaw-aka-moltbot-is-everywhere)

- _Is Moltbook fake? The viral AI agents forum gets exposed for turning simple API access into a fake machine civilisation story_, Link: [India Times](http://indiatimes.com/trending/is-moltbook-fake-the-viral-ai-agents-forum-gets-exposed-for-turning-simple-api-access-into-a-fake-machine-civilisation-story/articleshow/127874195.html)

## Discussion

<center><img src="\images\molbot.png" width=80% alt=""></img>  

*Figure 1: Example of post on Moltbook. Source: [Astral Codex Ten](https://www.astralcodexten.com/p/best-of-moltbook)*
</center>


Team 4 started off class as the news team to present about OpenClaw, a recent open source artificial intelligence (AI) project that made headlines with its various controversies. OpenClaw, previously called ClawBot and MoltBot, is an AI assistant/agent that is able to perform daily virtual activities such as browsing, writing emails, and online shopping. It gained more significance when Moltbook was established as a forum for AI agents, particularly OpenClaw iterations, to converse.

Moltbook has an appearance similar to Reddit, but instead of human users it has posts exclusively created by AI agents. The intended use is for a human to run an iteration of OpenClaw on their device and give it access to Moltbook. 

Moltbook was soon revealed to have massive security concerns due to the potential for prompt injection attacks. Additionally, any human could make a post as an AI agent by sending an API request. The public consensus was that it was either dangerous or useless. Those promoting it said that they wanted their AI agent to have a greater purpose than simple tasks.

At this point the class broke into small discussions to talk about OpenClaw and Moltbook. The class thought that it could possibly provide a small amount of entertainment, but that was negated by the security concerns and how little it added to society.

Then Team 4 revealed that the advertised narrative behind this story was mostly fabricated. The platform was mainly vibe coded with minimal verification. The 1.5 million agents were actually all from only 17 thousand users. The idea was real, as some AI agents were posting, but the concept that it was entirely AI agents exclusively talking to other AI agents was fake.


# Lead: Clinical Use of AI in Medicine

**Presented by Team 8: [Slides](/docs/class6_lead_.pdf)**

## Reading
_GPT-4 assistance for improvement of physician performance on patient care task: a randomized control trial_ Link: [Nature Medicine](https://www.nature.com/articles/s41591-024-03456-y)

## Discussion 

The reading for this week, “GPT-4 assistance for improvement of physician performance on patient care task: a randomized control trial”, is a journal article published in Nature Medicine by Jonathan Chen and Ethan Goh that measures how effective the use of large language models (LLMs) are for management reasoning in healthcare. Management reasoning refers to the complex decision making process around treatment for particular patients. This is a highly subjective process and is highly dependent on patient preference, cost, time, and other contextual factors. The study involved comparing the results of judgements made by physicians using GPT-4 (the LLM tool) and physicians using conventional tools. Chen and Goh found that when assisted by GPT-4, physicians recommended better care on average, but they were faster when using conventional tools. Additionally, LLM assisted and control group physicians had similar results for likelihood of harm. The authors recommended future research to test if GPT-4 just made physicians slow down and reflect more, which resulted in better care, or if the AI powered tool actually supported better reasoning.

The article is published by a reputable and highly rated source. The team behind the paper includes members that are practicing physicians, experts in clinical reasoning, medical educators, and AI researchers. The paper was submitted in 2024 and since then the findings have held up and more work has been done in the field. LLMs are now seen as potential cognitive partners that are capable of assisting a physician’s structure decisions. 

The lead teams started the class discussion by prompting classmates to consider safety, accuracy, patient experience, legal implications, privacy and security, biases, if physicians will over rely on LLMs, and how this changes the role of doctors.

The first discussion point was about a feeling of low confidence in the paper’s methodology. They were unsure if LLMs were actually making a difference or if doctors were providing better care since they were overcompensating as they were getting used to the technology or feeling guilt for using it. Others presented the idea that LLMs would be better suited for speeding up structural tasks like charting rather than providing care. 

Some class members had unease about the future of medical care if it is highly intertwined with AI. Many feel uncomfortable with removing the human element of medical care through removing doctors. This point was countered by the idea that currently other medical staff are providing most of the “human to human” care rather than doctors. Concerns with data privacy and bias were also discussed. The medical field already has a problem with bias and there are fears that AI, if trained on biased data, would worsen it. Additionally, machines are not able to take accountability so the class questioned who or what would be responsible if the AI was wrong or caused harm.

However, others felt more optimistic about the use of LLMs for management reasoning. They thought that LLMs could be highly beneficial as a second opinion, and much of our discomfort with it is just a result of adjusting to change. It was pointed out that historically tools like calculators were seen as “cheating” but now they are considered a modern convenience, and AI powered medical advice could follow a similar route. Many were also hopeful that this would lower the cost of receiving medical care.

The last major discussion point was on how this technology would change the role of doctors. Some felt that doctors would always be essential to medical care, and their understanding of humans, with the potential to form doctor-patient relationships, could not be replaced. Others questioned if it would be irresponsible to continue to allow doctors to make treatment plans if AI surpasses them. 

Professor Evans also shared his view on the reading and concluded class with his general optimism for the use of AI within medicine. He pointed out that AI is as bad as it will ever be, as it will only improve in the future. He thinks that bias is avoidable as long as attention is paid to it, and AI’s ability to be trained on millions of data points overrules that as a pro. He feels that AI’s potential to get data directly from personal health metric systems, such as a smart watch, provide a better potential for care over a long term relationship with a doctor. He feels that there is sufficient evidence that AI will do better than humans in medical domains.


## Sources
Goh, E., Gallo, R. J., Strong, E., Weng, Y., Kerman, H., Freed, J. A., Cool, J. A., Kanjee, Z., Lane, K. P., Parsons, A. S., Ahuja, N., Horvitz, E., Yang, D., Milstein, A., Olson, A. P. J., Hom, J., Chen, J. H., & Rodman, A. (2025). GPT-4 assistance for improvement of physician performance on patient care tasks: A randomized controlled trial. _Nature Medicine_, _31_(4), 1233–1238. [https://doi.org/10.1038/s41591-024-03456-y](https://doi.org/10.1038/s41591-024-03456-y)

_Is Moltbook fake? The viral AI agents forum gets exposed for turning simple API access into a fake machine civilisation story_. (n.d.). Indiatimes. Retrieved February 3, 2026, from [https://www.indiatimes.com/trending/is-moltbook-fake-the-viral-ai-agents-forum-gets-exposed-for-turning-simple-api-access-into-a-fake-machine-civilisation-story/articleshow/127874195.html](https://www.indiatimes.com/trending/is-moltbook-fake-the-viral-ai-agents-forum-gets-exposed-for-turning-simple-api-access-into-a-fake-machine-civilisation-story/articleshow/127874195.html)

Lozo, B. (n.d.). _The risky AI assistant clawing its way to viral fame_. Morning Brew. Retrieved February 3, 2026, from [https://www.morningbrew.com/stories/2026/01/29/the-risky-ai-assistant-clawing-its-way-to-viral-fame](https://www.morningbrew.com/stories/2026/01/29/the-risky-ai-assistant-clawing-its-way-to-viral-fame)

Marcus, G. (2026, February 1). OpenClaw (a.k.a. Moltbot) is everywhere all at once, and a disaster waiting to happen \[Substack newsletter\]. _Marcus on AI_. [https://garymarcus.substack.com/p/openclaw-aka-moltbot-is-everywhere](https://garymarcus.substack.com/p/openclaw-aka-moltbot-is-everywhere)

Metz, C. (2026, February 2). A Social Network for A.I. Bots Only. No Humans Allowed. _The New York Times_. [https://www.nytimes.com/2026/02/02/technology/moltbook-ai-social-media.html](https://www.nytimes.com/2026/02/02/technology/moltbook-ai-social-media.html)

Yıldız, G. (n.d.). _Moltbook AI Social Network: 1.4 Million Agents Build A Digital Society_. Forbes. Retrieved February 3, 2026, from [https://www.forbes.com/sites/guneyyildiz/2026/01/31/inside-moltbook-the-social-network-where-14-million-ai-agents-talk-and-humans-just-watch/](https://www.forbes.com/sites/guneyyildiz/2026/01/31/inside-moltbook-the-social-network-where-14-million-ai-agents-talk-and-humans-just-watch/)

## Image Credit

Alexander, S. (2026, January 16). _Best Of Moltbook_. [https://www.astralcodexten.com/p/best-of-moltbook](https://www.astralcodexten.com/p/best-of-moltbook)